{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Toxic Comment Classification</center></h1>\n",
    "\n",
    "\n",
    "# Introduction\n",
    "<p style='text-align: justify;'> With increased accessibility to the internet, communication has crossed all the barriers. Nowadays, people can share thoughts whenever, wherever with the entire world through social media. These comments not only reach people who favor them but also to people who are against them, and thus, give rise to open debates. Since the explicit interaction is with a computer or smartphone and not with humans, people tend to forget proper behaviour while commenting and type abusive toxic comments. Many times, people misbehave with this technology by creating fake accounts to spread hate and negativity.\n",
    "As it is said, diamond cuts a diamond, we want to use technology to reduce the abuse of technology. This project is about creating an Anti Toxic Comment Filter Bot using Data Science and Machine Learning methods, namely Natural Language Processing. NLP enables computers to process human language in the form of text or voice data and to ‘understand’ its full meaning, complete with the speaker or writer’s intent and sentiment.[1] The purpose of this bot is to identify threats and abusive comments, which are then categorized in six different categories based on the toxicity level. We want to enable this bot, in future, to be used as an extension with any social media to filter or hide out the toxic comments.</p>\n",
    "\n",
    "# Method\n",
    "<p style='text-align: justify;'>Therefore, classifying inappropriate comments can be a solution for a user friendly social platform. Here, around 160000 Wikipedia comments which have been labeled by humans into 6 different classes, like toxic, severe toxic, obscene, threat, insult, and identity hate are available from Kaggle competition.  Moreover, the correlation for each class is shown in picture below with pearson method. It is interesting to see that, “toxic” and “severe toxic” has less correlation compare to “toxic” with “obscene” or to “insult”. Higher correlation between “toxic” with “obscene” could be explained, that each sentence has both of labels. Furthermore, “threat” has less correlation with other classes compare to others as shown in image below.</p>\n",
    "<br />\n",
    "<p align=\"center\">\n",
    "    <img src=\"images\\matrix.PNG\">\n",
    "    <br>\n",
    "    <em>Image 1. Correlation of each label</em>\n",
    "</p>\n",
    "<br />\n",
    "<p style='text-align: justify;'>Moreover, the dataset has mostly “toxic” labels with around 15000 entries and the lowest is “threat” labels, with only around 1000 entries. This imbalance has to be taken into consideration before the classification step, because it influences how well the classification model performs. Many options, like upsampling/downsampling or data augmentation could be used to solve this imbalance.</p>\n",
    "<br />\n",
    "<p align=\"center\">\n",
    "    <img src=\"images\\class_dis.png\">\n",
    "    <br>\n",
    "    <em>Image 2. Class distribution</em>\n",
    "</p>\n",
    "<br />\n",
    "\n",
    "# Data preprocessing\n",
    "<p style='text-align: justify;'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ai_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61b7df5d9071b0efdf0b1f9d321606cf145668ae84e6d141964c63ae20698bd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
